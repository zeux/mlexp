{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.49977198243141174\n",
      "1 0.4782117009162903\n",
      "2 0.4656559228897095\n",
      "3 0.45816782116889954\n",
      "4 0.45353472232818604\n",
      "5 0.450512170791626\n",
      "6 0.44839954376220703\n",
      "7 0.44680219888687134\n",
      "8 0.44549739360809326\n",
      "9 0.44435951113700867\n",
      "10 0.4433174431324005\n",
      "20 0.434113472700119\n",
      "30 0.42534151673316956\n",
      "40 0.4168315827846527\n",
      "50 0.40857526659965515\n",
      "60 0.40056484937667847\n",
      "70 0.3927929997444153\n",
      "80 0.3852524161338806\n",
      "90 0.3779362738132477\n",
      "1000 0.1516803652048111\n",
      "2000 0.12985678017139435\n",
      "3000 0.1216125413775444\n",
      "4000 0.11443191766738892\n",
      "5000 0.10770104080438614\n",
      "6000 0.10136728733778\n",
      "7000 0.09540612250566483\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/zeux/mlexp/sinfit.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zeux/mlexp/sinfit.ipynb#W0sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m1e-4\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zeux/mlexp/sinfit.ipynb#W0sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m200000\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zeux/mlexp/sinfit.ipynb#W0sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39m# Forward pass: compute predicted y using operations on Tensors.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zeux/mlexp/sinfit.ipynb#W0sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m a \u001b[39m+\u001b[39m (b \u001b[39m*\u001b[39m x) \u001b[39m+\u001b[39m (c \u001b[39m*\u001b[39m x \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m+\u001b[39m (d \u001b[39m*\u001b[39m x \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m3\u001b[39m) \u001b[39m+\u001b[39m (e \u001b[39m*\u001b[39m x \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m \u001b[39m4\u001b[39;49m) \u001b[39m+\u001b[39m (f \u001b[39m*\u001b[39m x \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m5\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zeux/mlexp/sinfit.ipynb#W0sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39m# Compute and print loss using operations on Tensors.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zeux/mlexp/sinfit.ipynb#W0sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39m# Now loss is a Tensor of shape (1,)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zeux/mlexp/sinfit.ipynb#W0sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39m# loss.item() gets the scalar value held in the loss.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zeux/mlexp/sinfit.ipynb#W0sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     loss \u001b[39m=\u001b[39m (y_pred \u001b[39m-\u001b[39m y)\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_tensor.py:39\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[39m# See https://github.com/pytorch/pytorch/issues/75462\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[39mif\u001b[39;00m has_torch_function(args):\n\u001b[0;32m---> 39\u001b[0m         \u001b[39mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/overrides.py:1525\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Implement a function with checks for ``__torch_function__`` overrides.\u001b[39;00m\n\u001b[1;32m   1490\u001b[0m \n\u001b[1;32m   1491\u001b[0m \u001b[39mSee torch::autograd::handle_torch_function for the equivalent of this\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m...     return a + 0\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39m# Check for __torch_function__ methods.\u001b[39;00m\n\u001b[0;32m-> 1525\u001b[0m overloaded_args \u001b[39m=\u001b[39m _get_overloaded_args(relevant_args)\n\u001b[1;32m   1526\u001b[0m \u001b[39m# overloaded_args already have unique types.\u001b[39;00m\n\u001b[1;32m   1527\u001b[0m types \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mtype\u001b[39m, overloaded_args))\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/overrides.py:1466\u001b[0m, in \u001b[0;36m_get_overloaded_args\u001b[0;34m(relevant_args)\u001b[0m\n\u001b[1;32m   1459\u001b[0m arg_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(arg)\n\u001b[1;32m   1460\u001b[0m \u001b[39m# We only collect arguments if they have a unique type, which ensures\u001b[39;00m\n\u001b[1;32m   1461\u001b[0m \u001b[39m# reasonable performance even with a long list of possibly overloaded\u001b[39;00m\n\u001b[1;32m   1462\u001b[0m \u001b[39m# arguments.\u001b[39;00m\n\u001b[1;32m   1463\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   1464\u001b[0m \u001b[39m# NB: Important to exclude _disabled_torch_function_impl, otherwise\u001b[39;00m\n\u001b[1;32m   1465\u001b[0m \u001b[39m# https://github.com/pytorch/pytorch/issues/64687\u001b[39;00m\n\u001b[0;32m-> 1466\u001b[0m \u001b[39mif\u001b[39;00m (arg_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m overloaded_types \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(arg_type, \u001b[39m'\u001b[39m\u001b[39m__torch_function__\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   1467\u001b[0m         arg_type\u001b[39m.\u001b[39m__torch_function__ \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_disabled_torch_function_impl):\n\u001b[1;32m   1468\u001b[0m     \u001b[39m# Create lists explicitly for the first type (usually the only one\u001b[39;00m\n\u001b[1;32m   1469\u001b[0m     \u001b[39m# done) to avoid setting up the iterator for overloaded_args.\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m     \u001b[39mif\u001b[39;00m overloaded_types:\n\u001b[1;32m   1471\u001b[0m         overloaded_types\u001b[39m.\u001b[39madd(arg_type)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dtype = torch.float\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 1000, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "std = 1e-2\n",
    "a = torch.normal(0, std, (), dtype=dtype, requires_grad=True)\n",
    "b = torch.normal(0, std, (), dtype=dtype, requires_grad=True)\n",
    "c = torch.normal(0, std, (), dtype=dtype, requires_grad=True)\n",
    "d = torch.normal(0, std, (), dtype=dtype, requires_grad=True)\n",
    "e = torch.normal(0, std, (), dtype=dtype, requires_grad=True)\n",
    "f = torch.normal(0, std, (), dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(200000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + (b * x) + (c * x ** 2) + (d * x ** 3) + (e * x ** 4) + (f * x ** 5)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).mean()\n",
    "    if t < 10 or (t < 100 and t % 10 == 0) or t % 1000 == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "        e -= learning_rate * e.grad\n",
    "        f -= learning_rate * f.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "        e.grad = None\n",
    "        f.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3 + {e.item()} x^4 + {f.item()} x^5')\n",
    "\n",
    "\n",
    "_y = []\n",
    "\n",
    "for _x in x:\n",
    "    res = math.sin(_x)\n",
    "    approx_res = a + (b * _x) + (c * _x ** 2) + (d * _x ** 3) + (e * _x ** 4) + (f * _x ** 5)\n",
    "    _y.append(approx_res)\n",
    "\n",
    "\n",
    "plt.plot(torch.Tensor.cpu(x), torch.Tensor.cpu(y), color='r', label='sin')\n",
    "plt.plot(torch.Tensor.cpu(x), torch.Tensor.cpu(torch.tensor(_y)), color='g', label='approx')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title(\"Sin Approx\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
